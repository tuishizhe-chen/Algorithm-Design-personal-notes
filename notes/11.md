## Lecture 11

**Project Selection**, **Baseball Elimination**, and **Parametric Flow** are three basic applications of network flow.

### Min-cost Flow/ Min-cost Matching
#### Min-cost Matching
given a bipartite graph and aim for the matching that minimizes the total cost of the matching edges. There is a reduction to the max-flow problem.
#### Min-cost Flow
A generalization of the max-flow problem. $C_e$ are the capacities $W_e$ are the costs. The goal is to minimize $\sum_e{w_ef(e)}$ subject to the constraint that $\sum_e{f(e)}\ge F$, where $f$ is a flow. **Note that this problem is strongly polynomial.** Also, the min-cost circulation problem is also polynomial-time solvable
#### Max-Matching problem in a general graph

### Chernoff Bound

Chernoff Bound provides a probabilistic bound on the sum of independent random variables. It is useful for analyzing the performance of randomized algorithms.

#### Theorem
Let $X_1, X_2, \dots, X_n$ be independent random variables taking values in $\{0, 1\}$. Let $X = \sum_{i=1}^{n} X_i$ and $\mu = \mathbb{E}[X]$. Then for any $\delta > 0$:

$$\Pr[X \ge (1 + \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2 + \delta}\right)$$

and

$$\Pr[X \le (1 - \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2}\right)$$

These bounds show that the probability of the sum deviating significantly from its expected value decreases exponentially with the size of the deviation.

***The following proof is generated by ChatGPT, or probably github copilot, 看个乐呵就行***

### Chernoff Bound Proof

Chernoff Bound provides a probabilistic bound on the sum of independent random variables. It is useful for analyzing the performance of randomized algorithms.

1. **Introduce the Moment Generating Function and Markov's Inequality**:

 For any $t > 0$, we have: $\Pr[X \ge (1 + \delta)\mu] = \Pr[e^{tX} \ge e^{t(1 + \delta)\mu}]$. By Markov's Inequality: $\Pr[e^{tX} \ge e^{t(1 + \delta)\mu}] \le \frac{\mathbb{E}[e^{tX}]}{e^{t(1 + \delta)\mu}}$

2. **Calculate $\mathbb{E}[e^{tX}]$**:

Since $X = \sum_{i=1}^n X_i$ and $X_i$ are independent, we have: $\mathbb{E}[e^{tX}] = \mathbb{E}\left[e^{t \sum_{i=1}^n X_i}\right] = \prod_{i=1}^n \mathbb{E}[e^{t X_i}]$. For each $X_i$, we have: $\mathbb{E}[e^{t X_i}] = p e^t + (1 - p) = 1 + p(e^t - 1)$.

Therefore: $\mathbb{E}[e^{tX}] = \left(1 + p(e^t - 1)\right)^n$

3. **Combine Markov's Inequality and the Moment Generating Function**:

 Substitute $\mathbb{E}[e^{tX}]$ into Markov's Inequality: $\Pr[X \ge (1 + \delta)\mu] \le \frac{\left(1 + p(e^t - 1)\right)^n}{e^{t(1 + \delta)\mu}}$

4. **Optimize the Parameter $t$**:

 Choose $t = \ln(1 + \delta)$, we have: $\Pr[X \ge (1 + \delta)\mu] \le \frac{\left(1 + p((1 + \delta) - 1)\right)^n}{e^{(1 + \delta)\mu \ln(1 + \delta)}}$.

 Simplifying, we get: $\Pr[X \ge (1 + \delta)\mu] \le \left(\frac{e^\delta}{(1 + \delta)^{1 + \delta}}\right)^\mu$. Further simplification gives: $\Pr[X \ge (1 + \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2 + \delta}\right)$

5. **Similarly, we can prove $\Pr[X \le (1 - \delta)\mu]$**:

 Using a similar method, we can prove: $\Pr[X \le (1 - \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2}\right)$. Thus, we have proved the Chernoff Bound.
