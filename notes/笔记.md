# Algorithm Design 
## Lecture 4
### **Problem 1** is the traditional interval problem. 

We can use $OPT(j)$ to represent the optimal arrangement of works in the scope of ${1,2,\dots,n}$, then $OPT(j)=\max\{v_j+OPT(P(j)), OPT(j-1)\}$, where $P(j)$ represents the max index $k$ such that the ending time of $I_k$ is earlier than the beginning time of $I_j$.

### **Problem 2** is to find the best approximation of given $n$ points with segments, i.e., split the $n$ given points into $k$ continuous fractions, and use $k$ segments to approximate each fraction. Our goal is to minimize $Ck+\sum_{i=1}^{k}error(\text{segment}_i)$. 

*The algorithm*: In fact, you can use $OPT(j)$ to represent the best approximate error of the first $j$ points, then $OPT(j)=\max_{1\le k\le j}\{OPT(k-1)+C+error(k,j)\}$, where $error(k,j)$ represents the best approximate error of points $k,\dots,j$ with one segment which can be calculated directly.

**Projective clustering** is another problem that might be worthy to study.
  
### **Problem 3** Subset sum & knapsack

**Subset sum:** given a set $A$ consisting of $n$ items with their weights $w_i$. Input a value $W$, figure out whether there exists a subset of $A$ whose total weight of its items is $W$.

**Knapsack:** additionally, each item has a value, our goal is to ensure that the total weight of a subset is less or equal to $W$ and maximize the total value of this subset.

- In fact, the running time can be $poly(n,W)$ when $w_i$'s are non-negative integers. However, without the assumption, they are NP-Complete and NP-Hard because the input scale is actually $O(n\log W)$. The running time of the following algorithm is pseudo-polynomial time.

- *The algorithm* uses $OPT(j,W)$ to represent the optimal solution for capacity $W$ with items with index less or equal to $j$, then $OPT(j,W)=\max \{OPT(j-1,W), OPT(j-1,W-w_j)+v_j\}.

### **Problem 4** Shortest path with possibly negative edge weights without negative cycles.

*Bellman-Ford Algorithm*'s method is DP. A natural method is to use $OPT(v,i)$ to represent the shortest path from $v$ to $t$ with less or equal to $i$ edges. In fact, $OPT(v,i)=\min\{\min _{v\rightarrow u}\{C_{vu}+OPT(u,i-1)\}, OPT(v,i-1)\}.

- *Algorithm analysis* The time complexity is $O(nm)$ and the space complexity is $O(n).

The real *Bellman-Ford Algorithm* is more elegant. We can use $M[v]$ to denote the shortest path from $v$ to $t$, then we can use $M[v]=\min \{M[v],\min _{v\rightarrow u}\{C_{vu}+M[u]\}\}. After $i$ iterations, $M[v]$ is in fact less or equal to $OPT(v,i).

### **Problem 4'** Additionally, we not only need to output the length of the shortest path, but also need to output the shortest path itself.

Maintain an array `First[u]` to be the first vertex after $u$ on the recent shortest path from $u$ to $t`. Whenever we update $M[u]` with $C_{uv}+$`M[v]`, let `First[u]` be `v`.

*Proof*: Let $P$ be the pointer graph generated by `First[u]`, then it's easy to illustrate that if there is a cycle in $P$ at any stage, then the total cost of this cycle must be negative. 

## Lecture 5
### Tree decomposition and tree-width
***Tree decomposition*** for graph $G$ is a corresponding tree $T$ whose vertices are sets consisting of vertices of $G$, let $t$ s denote vertices of $T$, and $V_t$ denotes the corresponding set of $t$. $T$ should satisfy the following properties:
- each vertex is contained by some $T_t$ s
- each edge is contained by some $T_t$ s
- (*Coherence*) let $t_1,t_2,t_3\in T$, where $t_2$ lies on the unique path from $t_1$ to $t_3$. If $v\in V_{t_1}\cap V_{t_3}$, then $v\in V_{t_2}$
  
Tree decomposition is not unique, we use $tw(T)$ to denote $\max_{t\in T}(|V_t|-1)$, then the ***tree width*** of $G$ is defined by $\min_{T \text{ is a decomposition}}tw(T).

Here are two lemmas:

- Suppose $T-t$ has components $T_1,\dots,T_k$, then $G_{T_1},\dots,G_{T_k}$ have no nodes in common and no edges between them.
- Suppose $t_1,t_2\in T$ are two adjacent bags, then deleting $V_{t_1}\cap V_{t_2}$ disconnects $G$ into more than 1 component.

Unfortunately, $\min(tw(T))$ is NP-hard.

### Max-weight independent set with tree decomposition

Let $T$ be a tree decomposition of $G$, direct $T$ with root $\tau$, for an arbitrary vertex $t\in T$, let $T_t$ be the subtree of $T$ with root $t$. Let $G_t$ or $G_{T_t}$ be the sub-graph of $G$ which is generated by all vertices in some $V_{t'}$s in $T_t$. For an arbitrary independent set $U\subset V_t$, let $f_t(U)$ denote the max-weight independent set $S$ of $G_t$, subject to the requirement that $S\cap V_t=U$.

We can use the **DP-method** to compute $f_t(U)$. In fact, 
$f_t(U)=w(U)+\sum_{t_i}\max(f_{t_i}(U_i):U_i\cap V_t=U\cap V_{t_i}, U_i\text{ is IS of } T_{t_i})$, where $t_i$ s are all sons of $t$ in $T$.

### Minor 
***Minor***: a minor of a graph $G$ can be obtained by the following operations:
- **Contraction:** For connected vertices $u,v$, delete them and create a new vertex $w$ whose $N(w)=N(u)\cap N(v)$
- **Deletion:** delete a vertex (and its corresponding edges) or an edge 

A graph family $\mathcal{F}$ is called ***minor-closed*** if $G\in\mathcal{F}\rightarrow \text{minors of }G\in\mathcal{F}$.

### NP-Completeness
We say **$Y\le_pX$**, if $Y$ is poly-time solvable with the assumption that $X$ is poly-time solvable.

Example:

**Set cover (SC):** $U$ with a family $\mathcal{F}$ of $U$'s subsets. Whether there is a collection $\mathcal{C}$ of $\mathcal{F}$ with size $k$ such that the union of sets in $\mathcal{C}$ is the entire $U$.

VC(vertex cover) $\le_p$ SC.

## Lecture 6
### NP-Completeness
Here are several problems that are NP-Complete as follows
### SAT Problem
#### 3-SAT Problem
#### Independent Set Problem
#### Vertex Cover Problem
#### Dominating set Problem
#### Hamiltonian Cycle Problem
In both directed and undirected graphs
#### 3-Dimensional Matching Problem
#### 3-Coloring Problem
#### Subset Sum Problem
#### Max 2-SAT Problem

## Lecture 7

### FPT running time 
- Running time of $O(f(k)\cdot\text{poly}(n))$, where $k$ is a parameter.
- Here is an FPT algorithm for the ***Vertex Covering Problem*** (decision version).
  - Observation: Suppose that $e=(u,v)$ is an edge of $G$, then $VC(G)\le k$ is equivalent to $VC(G-u)\le k-1\lor VC(G-v)\le k-1$
  - **The algorithm**: 
    ```plaintext
        function VertexCover(G, k):
        if k < 0:
            return False
        if G has no edges:
            return True
        if |E(G)|> k|V|
        choose an edge (u, v) in G
        return VertexCover (G - u, k - 1) or VertexCover(G - v, k - 1)
    ```
  - The running time: $T(n,k)\le 2T(n-1,k-1)+O(nk)$, so $T(n,k)\le 2^{k+1}nk$. 
  - The best FPT for vertex covering can achieve $1.2738^k\cdot\text{poly}(n)$.
  
### Approximation Algorithms
**The definition of Approximation ratio/factor:**
  $$\alpha= \sup_{I}\frac{\text{SOL}(I)}{\text{OPT}(I)}$$
   for minimization problems and 
  $$\alpha= \sup_{I}\frac{\text{OPT}(I)}{\text{SOL}(I)}$$ 
  for maximization problems.

***PTAS (poly-time approximation scheme)***: Can achieve $1+\epsilon$ approximation factor with polynomial running time.

#### Load Balancing Problem
Description: $n$ jobs (job $j$ has processing time $t_j$) and $m$ machines, our goal is to minimize $\max_{i}(\sum_{j \text{assigned to} M_i}t_j)$.

Load Balancing Problem is NP-hard due to the ***Subset Sum Problem***.

Here is a greedy algorithm to achieve *PTAS* in Load Balancing Problem: process jobs one-by-one from the smaller $t_j$ to the bigger $t_j$. Assign the jobs to the machines with the minimum load.

***Analysis:*** We have the following observations (Let $T*$ be the optimal one):
-  $T*\ge\max{t_j}$
- $T*\ge\frac{\sum_{j=1}^{n}t_j}{m}$

Therefore $\text{Solution}\le\frac{\sum_{j=1}^{n}t_j}{m}+\max{t_j}\le 2T*$

The analysis above is ***tight***, i.e., $2$ is the best approximation factor for this algorithm. It's quite easy to give an example.

The ***tightness*** of the analysis is not the same as "The Approximation factor is optimal". In fact, whether the factor $2$ can be optimized is part of ***Inapproximability theory***, a brief understanding of this theory is like "achieving a better approximation factor is NP-hard".

***Improved approximation factor:*** processing jobs one by one from the biggest $t_j$ to the smallest.

**Analysis:** Assume the machine $M_i$ has the biggest load. Let $B$ be the processing time of the job which is lastly added to $M_i$, then $T*=\text{Solution}$ when $M_i$ only has one job. Now we assume that there are at least 2 jobs in $M_i$. Then consider the first $m+1$ jobs, obviously the optimal max-load is at least $t_{m}+t_{m+1}\ge 2t_{m+1}\ge 2B$. Therefore, $\text{Solution}\le B+\frac{\sum_{j=1}^{n}t_j}{m}\le \frac{3}{2}T*$.

#### $k-$center problem: 
Given a metric graph, choose $k$ vertices to form the center $C$ to minimize $\max_{v\in V}d(v,C)$.

Here is an algorithm providing an approximation factor of 2:

```
Guess the optimal solution r (there are at most |E| guesses)
  S = G, C is empty
    while S is not empty:
      choose an arbitrary vertex from S
            C = C + v
            delete all the nodes in V with distance to v less or equal than 2 * r
    if: |C| <= k
      return succeed
```

We claim that if our guess is correct ( $\ge$ OPT), then the algorithm will return succeed, because all the vertices in the territory of one center vertex will be deleted from `S` whenever a vertex in this territory is selected to be the `v`.

The algorithm's approximation factor is $2$.

**Optimality**: 2 is the best approximation factor, which means a $(2-\epsilon)$ approximation algorithm can be used to solve the *3-SAT Problem*.

#### Knapsack Problem


***Algorithm:*** we discretize the value at first: Let $b=\frac{\epsilon}{2n}\max_{w_i\le W}{v_i}$, and let $\hat{v_i}=\lceil \frac{v_i}{b}\rceil$. Then DP for the total value (after discretization) and output the greatest total value for which the mini weight is not greater than $W$.

**Analysis:** We can achieve an approximation factor of $(1+\epsilon)$.  In fact, if the optimal solution is $\{(w^{*}i,v^{*}i)\}$ and our algorithm generated $\{(w_i,v_i), i=1,\dots,n\}$. Then we have that 
$$\sum_{i=1}^{t}v_i = \sum_{i=1}^{t}(b\cdot\hat{v_i}-b\cdot(1-\{\frac{v_i}{b}\}))\ge \sum_{j=1}^{t*}b\cdot\hat{v*j}-tb\ge OPT-nb$$
Note that $OPT\ge \frac{2nb}{\epsilon}$, therefore 
$$Sol\ge OPT\cdot(1-\frac{\epsilon}{2})$$ hence the approximation factor is about $1+\epsilon$


## Lecture 8
### Set Cover problem 
$U$ is a set of elements, there are subsets $S_1,\dots,S_m\subset U$ with weights $w_1,\dots,w_m$. The requirement is to find out the set cover with minimum total weight.

***Algorithm*** we implements greedy method, select the new subset with minimum $\frac{w_i}{|\text{new elements contained by }S_i|}$. 

#### Analysis 
Suppose that $S_{i_1},\dots,S_{i_k}$ are the subsets selected by the algorithm in order. Assume that they cover $m_j$ new elements, respectively. Assume that after select the first $j$ subsets $S_{i_1},\dots,S_{i_j}$, there are $n_j$ remained elements. We claim that 
$$\frac{w_{i_{t+1}}}{m_{t+1}}\le \frac{OPT}{n_t}$$
Therefore, $$Solution=\sum{w_{i_t}}\le \sum_{t=1}^{k}\frac{OPT}{n_{t-1}}m_{i_t}=OPT\cdot(\frac{m_1}{n_0}+\frac{m_2}{n_1}+\dots)\\ \le OPT\cdot(\frac{1}{n}+\dots+\frac{1}{n-m_1+1}+\dots)=OPT\cdot(1+\frac{1}{2}+\cdots+\frac{1}{n}) $$
*Note*: It's NP-hard to approximate ***Set Cover Problem*** with a factor of $(1-\epsilon)\log n$

### Linear Programming 
minimize/maximize $2x_1-x_2$ in constraints of some linear inequalities of $x_1,x_2$. A formal description of this problem is minimize/maximize $C^{T}x$ subject to $Ax\le b$, where $x,C\in \mathbb{R}^n$, $A\in M_{m\times n}(\mathbb{R}), b\in\mathbb{R}^m$.

A geometrical understanding of linear programming is $x$ be contained in the intersection of several halfspaces, which is in fact a polyhedron and to minimize/maximize the distance $x$ to a supersurface.

#### Vertex Cover 
there is a natural transform from vertex cover problem to a linear programmin: let variables $x_1,\dots,x_n\in\{0,1\}$ decide the selection: If we choose $v_i$, then set $x_i$ to $1$. We need to minimize $\sum_{i}w_ix_i$ with subject to $x_i+x_j\ge 1$ for arbitrary connected vertices $v_i,v_j$. However, this is a **Integrating Linear Programming(ILP)**, we need to do **LP-relaxation(LPR)** as follow: $\hat{x_i}=\lfloor x_i+0.5\rfloor$. 

It is obvious that this approximating problem is feasible. Considered that $Sol=\sum{w_i\hat{v_i}}\le\sum{2w_iv_i}=2OPT(LPR)\le 2OPT(ILP)$, therefore we can achieve approximate factor of 2.        

#### Set Cover 
Here is a random algorithm based on the method of the *Vertex Cover Problem*. Let $x_1, x_2, \dots, x_m$ be variables in the range of $(0,1)$ that correspond to $m$ sets. We use a linear programming technique to minimize $\sum_{i=1}^{m} x_i w_i$, where $w_i$ are the weights of the sets, subject to $\sum_{e \in S_i} x_i \ge 1$ for any element $e$. Suppose $(x_1, \dots, x_m)$ is the optimal solution for this linear programming problem. We select $S_i$ with probability $x_i$ and repeat this probabilistic selection process $n$ times. We know that the probability that $e$ is not contained in any selected set is:

$$\prod{(1-x_i)^n} = \exp\left(\sum n \log (1-x_i)\right) \le \exp\left(n \cdot \sum (-x_i)\right) \le e^{-n}$$

Therefore, the probability that there exists an element that is not contained in any selected set is less than or equal to $e^{-n} \cdot |S|$, where $S$ is the union of all candidate sets. We set $n$ to be $\log (|S|^2)$, and the success probability is optimal enough. Morever, the approximating ratio is about $O(\log |S|)$ since the optimal solution's total weight is not greater than $\sum x_i$ of expectancy.
#### Max-cut Problem 
#### TSP (Travelling salesman)

similar with Hamilton cycle problem, but visit every vertex at least once, minimize the total length.

***2-Approximating algorithm*** 
Select a generated tree with smallest total weight. We only need to show that we can construct a path such that it traverse all vertices and the total weight is less or equal than 2 times the total weight of the tree selected above. 

we copy the minimum spanning tree and double the edges. Then we can find a Euler cycle in this graph (This is mathematically trivial, and we can implement this by induction). The Euler cycle is a path that traverse all vertices with total weight less or equal than 2 times the total weight of the minimum spanning tree, therefore less or equal than 2 times the optimal solution.
  



## Lecture 9 
### Divide and Conquer
An instance for ***Devide and Conquer*** is reduction sorting algorithm
#### Closest Pair Problem 
Our method is to devide the points to two parts and compute the minimum distance in each fraction respectively and compute the minimum distance that cross two fractions. 
Here is part of the seudocode:
```cpp
while (!V.empty()) {
  // Select a line that splits V into two fractions
  // Let d1 be the closest distance in the left half
  // Let d2 be the closest distance in the right half
  // Compute the closest pair that crosses the line
}
```
Computing the closest pair that crosses the line seems to be difficult. Let $\delta$ the smallest one among `d1` and `d2`. we divide the square into size $\delta/2$ subsquares, where the division line $l$ overlapping with edges of some subsquares. 
Here is a figure to illustrate the method:

<!-- insert a figure -->


The key observation is that there is at most one point in each subsquare, therefore for arbitrary point $A$, we only need to check about $O(1)$ subsquare to figure out if there exists a point such that it is less than $\delta$ away from $A$ and record the shortest distance of $A$ with points in the other fraction. Thus the computation of whether there exists a pair of points crosses the line with distance not greater than $\delta$, and what is the closest pair if there really exists only cost $O(n)$ times.

#### Select the kth element
##### The randomized algorithm
To select the k-th smallest element from an unsorted array, we can use the **Divide and Conquer** method. This approach is similar to the QuickSort algorithm.
***Algorithm:***
1. **Partition** Randomly select a pivot position to divide the elements into 2 fractions, this process cost $O(m)$ times, where $m$ is the availible elements recently. 
2. **Recur**:
  - If the pivot position is the k-th position, return the pivot.
  - If the k-th position is less than the pivot position, recursively apply the algorithm to the left subarray. Similar recursion when the k-th position is greater than the pivot position.
  
***Analysis:***
We say that a recursion is *helpful* if the size of the subarray is less than $\frac{3}{4}$ times of the array. It is obvious that the probability that a recursion is *helpful* is at least $\frac{1}{2}$. Therefore, we record the milestones $M_i$s that the first time the subarray size is less than $n\cdot (\frac{3}{4})^{i}$. Then expected number to successful iterations of reach $M_i$ from $M_{i-1}$ is less than $\sum_{t=1}^{\infty}(\frac{1}{2}^t\cdot O(t))=O(1)$, therefore the expected time is less or equal than 
$$\sum_{i=1}^{O(\log n)}(\frac{3}{4})^{i-1}O(n)\cdot\mathbb{E}(\text{iterations that reach } M_i\text{ from }M_{i-1})=O(n)$$

##### The determined algorithm
We divide the elements into $\frac{n}{5}$ groups. Select the median of each group, and use the median of the medians to be the pivot position. 

***Complexity Analysis***
$$f(n)\le O(n)+f(\frac{n}{5})+\max\{f(\frac{3}{10}n), f(\frac{7}{10}n)\}\le O(n)+f(\frac{n}{5})+f(\frac{7}{10}n)$$
Therefore the complexity is $O(n)$

#### Convolution and FFT
***Convolution:*** Input $(a_1,\dots,a_n), (b_1,\dots,b_n)$. Then we define $a*b$ as 
$$(a_0b_0, a_0b_1+a_1b_0, a_0b_2+a_1b_1+a_2b_0, \dots,a_{n-1}b_{n-1})$$ 
In fact, we can view convolution as polynomial multiplication. Our goal is to compute this in $O(n\log n)$ times.

***Algorithm:*** Assume that $n=2^h$ for some positive integer $h$. We select $2n$ points $x_1,\cdots,x_{2n}$, processing the steps as follows:
- Compute $A(x_1), \dots, A(x_{2n}), B(x_1),\cdots, B(x_{2n})$
- $C(x_j)=A(x_j)B(x_j)$
- Reconstruct $C$ from $C(x_j)$s

We let $x_j$ be $e^{\frac{i\cdot 2j\pi}{2n}}$.
1. We denote $a_0+a_2x+\cdots+a_{n-2}x^{\frac{n-2}{2}}$ as $A_{even}(x)$ and $a_1+a_3x+\cdots+a_{n-1}x^{\frac{n-1}{2}}$ as $A_{odd}(x)$. Therefore 
  $$A(x)=A_{even}(x^2)+x\cdot A_{odd}(x^2)$$

## Lecture 10
### Network Flow
The definition of flow is as follows:
- Directed graph $G(V,E)$
- each edge has a capacity $C_e\ge 0$
- Source node $s$, Sink node $t$
- Flow $f$ is a function from $E$ to $\mathbb{R}$, subject to the following constraints:
  - Capacity constraint: $0\le f(e)\le C_e$
  - Flow conservation: $\sum_{e\in\delta^{+}(v)}f(e)=\sum_{e\in\delta^{-}(v)}f(e)$ for all $v\in V-\{s,t\}$
#### Max Flow Problem
The **Max Flow** problem is to maximize 
$$V(f)=\sum_{e \text{ out of }s}f(e)$$
##### S-T Cut
For a cut $C=(C_s,C_t)$, we define the value capacity of the cut as 
$$V(C)\sum_{u\in C_s,v\in C_t}C_{uv}$$
**max-flow minicut THM** claims that $$\min_{C\text{ is a cut }}V(C)=\max_{f\text{ is a flow }V(f)}$$

##### Ford-Fulkerson Algorithm
For a flow $f$, we define the residual graph $G_f$ to be the graph that
-  share the same vertex set and edge set with $G$
- The capacity of $e$ in $G_f$ is $C_e-f(e)$ for $e\in E$
- Moreover, for arbitrary edge $e=(u,v)\in E$ such that $f(e)<C_e$, we create a new edge $e'=(v,u)$ with capacity in $G_f$ of $f(e)$, and call it a **backward edge**, and call $e$ itself a **forward edge**.

The algorithm is to push flows into $f$ and update the residual graph until it's impossible to push flows anymore. More precisely, initialize $f$ to $e\mapsto 0$ at first. Then we repeat the following process:
- Find a path from $s$ to $t$ in $G_f$ and let $\delta$ be the minimum capacity along this path.
- Update $f$ for edges along the path: $f(e)\leftarrow f(e)+\delta$ for forward edges, $f(e)\leftarrow f(e)-\delta$ for backward edges.
- update $G_f$.
- If there doesn't exist a path from $s$ to $t$ in $G_f$, then output $f$.

Here we show that ***Ford-Fulkerson*** algorithm is feasible, Note that *FF* algorithm is not necessarily terminate but we can prove it with the assumption of integer capacities. Note that the **time complexity** of this algorithm is $O(|E|\sum_{e}C_e)$, which is a pseudo-polynomial time.

**Feasibility** Denote that $f$ is the flow obtained by the FF algorithm. Suppose that $S\subseteq V$ be the set contains all the vertices that can be reached from $s$ in $G_f$. Therefore we have that 
$$V(f)=\sum_{(s,u)\in E}f(s,u)=f^{\text{out}}(S)-f^{\text{in}}(S)=f^\text{out}(S)=V(S,V-S).$$
Therefore, consider that $V(f)\le V(C_s,C_t)$ for arbitrary $f,C$, we deduce that $V(f)=\min_{f'\text{ is a flow}}V(f')=\min_{C\text{ is a cut}}V(C)=V(S,V-S)$, hence that $f$ is a max flow.
#### Bipartite Matching
#### Circulation Problem
This problem is similar with the max flow problem. There are differences as follows:
- multiple sources and sinks: There is a demand for every node $v$: $d_v=f^\text{in}(v)-f^\text{out}(v)$, $v$ is a source iff $d_v<0$ and $v$ is a sink iff $d_v>0$.
- capacity constraint: $f(e)\in[l_e,C_e]$
- The goal is to find a feasible flow/circulation

The algorithm is easy: We firstly get rid of the capacity constraints by simply minus $l_e$ from $C_e$ and obtain new demands. Therefore we can solve this problem by adding super source and super sink, applying the max-flow problem.

## Lecture 11
### Image Segmentation

### Project Selection
### Baseball Elimination
### Parametric Flow
### Min-cost Flow/ Min-cost Matching
#### Min-cost Matching
given a bipartite graph and aim to the mathing that minimize the total cost of the matching edges. There is a reduction to max-flow problem.
#### Min-cost Flow
A generalization of max-flow problem. $C_e$ are the capacities $W_e$ are the costs. The goal is to minimize $\sum_e{w_ef(e)}$ subject to the constraint that $\sum_e{f(e)}\ge F$, where $f$ is a flow. **Note that this problem is strongly polynomial.** Also, min-cost circulation problem is also polynomial-time solvable
#### Max-Matching problem in general graph

### Chernoff Bound

Chernoff Bound provides a probabilistic bound on the sum of independent random variables. It is useful for analyzing the performance of randomized algorithms.

#### Theorem
Let $X_1, X_2, \dots, X_n$ be independent random variables taking values in $\{0, 1\}$. Let $X = \sum_{i=1}^{n} X_i$ and $\mu = \mathbb{E}[X]$. Then for any $\delta > 0$:

$$\Pr[X \ge (1 + \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2 + \delta}\right)$$

and

$$\Pr[X \le (1 - \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2}\right)$$

These bounds show that the probability of the sum deviating significantly from its expected value decreases exponentially with the size of the deviation.

### Chernoff Bound Proof

Chernoff Bound provides a probabilistic bound on the sum of independent random variables. It is useful for analyzing the performance of randomized algorithms.

#### Theorem
Let $X_1, X_2, \dots, X_n$ be independent random variables taking values in $\{0, 1\}$. Let $X = \sum_{i=1}^{n} X_i$ and $\mu = \mathbb{E}[X]$. Then for any $\delta > 0$:

$$\Pr[X \ge (1 + \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2 + \delta}\right)$$

and

$$\Pr[X \le (1 - \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2}\right)$$

#### Proof

1. **Introduce the Moment Generating Function and Markov's Inequality**:

   For any $t > 0$, we have:

   $$
   \Pr[X \ge (1 + \delta)\mu] = \Pr[e^{tX} \ge e^{t(1 + \delta)\mu}]
   $$

   By Markov's Inequality:

   $$
   \Pr[e^{tX} \ge e^{t(1 + \delta)\mu}] \le \frac{\mathbb{E}[e^{tX}]}{e^{t(1 + \delta)\mu}}
   $$

2. **Calculate $\mathbb{E}[e^{tX}]$**:

   Since $X = \sum_{i=1}^n X_i$ and $X_i$ are independent, we have:

   $$
   \mathbb{E}[e^{tX}] = \mathbb{E}\left[e^{t \sum_{i=1}^n X_i}\right] = \prod_{i=1}^n \mathbb{E}[e^{t X_i}]
   $$

   For each $X_i$, we have:

   $$
   \mathbb{E}[e^{t X_i}] = p e^t + (1 - p) = 1 + p(e^t - 1)
   $$

   Therefore:

   $$
   \mathbb{E}[e^{tX}] = \left(1 + p(e^t - 1)\right)^n
   $$

3. **Combine Markov's Inequality and the Moment Generating Function**:

   Substitute $\mathbb{E}[e^{tX}]$ into Markov's Inequality:

   $$
   \Pr[X \ge (1 + \delta)\mu] \le \frac{\left(1 + p(e^t - 1)\right)^n}{e^{t(1 + \delta)\mu}}
   $$

4. **Optimize the Parameter $t$**:

   Choose $t = \ln(1 + \delta)$, we have:

   $$
   \Pr[X \ge (1 + \delta)\mu] \le \frac{\left(1 + p((1 + \delta) - 1)\right)^n}{e^{(1 + \delta)\mu \ln(1 + \delta)}}
   $$

   Simplifying, we get:

   $$
   \Pr[X \ge (1 + \delta)\mu] \le \left(\frac{e^\delta}{(1 + \delta)^{1 + \delta}}\right)^\mu
   $$

   Further simplification gives:

   $$
   \Pr[X \ge (1 + \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2 + \delta}\right)
   $$

5. **Similarly, we can prove $\Pr[X \le (1 - \delta)\mu]$**:

   Using a similar method, we can prove:

   $$
   \Pr[X \le (1 - \delta)\mu] \le \exp\left(-\frac{\delta^2 \mu}{2}\right)
   $$

Thus, we have proved the Chernoff Bound.

## Lecture 12

## Lecture 13
### Streaming Algorithms
a stream $x_1,\cdots,x_n$, while the space is limited (such as $O(\sqrt{n})$ or $poly\log(n)$)

#### Count Distinct Elements
- The performance of $poly\log$ space and $(\epsilon, \delta)$ approximation can be reached.

**(ideal) hash functions**: Suppose that we have an ideal hash function $h: [m]\rightarrow [0,1]$

Note that if we record the minimum of the hash function result, the expected value of it is $\frac{1}{F_0+1}$, where $F_0$ indicates the number of distinct elements.

Moreover, we maintain the $t=O(\frac{1}{\epsilon^2})$ smallest hash values. Let $v$ be the $t$ th value and output $\tilde{F_0}=\frac{t}{v}$

###### Lemma
$\Pr[\tilde{F_0}<(1-\epsilon)F_0]<\frac{1}{100}$

*Proof*: Let $X_i$ indicates if $h(x_i)\le\frac{t}{F_0(1-\epsilon)}$ and denote that $Y=\sum_{X_i}$, thus we only need to handle $\Pr[Y<t]$

In fact, $\Pr[Y<t]\le\frac{\text{Var}how[Y]}{\epsilon^2t^2}$

#### Bloom Filter

Bloom Filter can handle membership with space much less than the size of set while tradeoff the accuracy.

there are many hash functions $T_1,\cdots, T_k$, each hash value has  a corresponding key boolean value. Whenever $y$ is push into $S$, the key value corresponding to $T_1(h(y)), \cdots, T_k(h(y))$ are set to 1. The output of $y$ is check whether $T_i(h(y))=1$, $\forall i$.

#### min-wise hashing

##### LSH (locality sensitive hashing)

LSH can approximate nearest neihbor search in high dimension

##### Dimension Reduction

Essential algorithm: Johnson-Lindenstrauss

### Local Search

Neiborhood relation over all solutions

local search can be viewed as preforming a walk on the relationship graph

#### Gradient Descent

#### Metropolis Algorithm and Simulated Annealing

Statistical Machanics : $\Pr[a \text{ state with energy } E]\sim \exp(\frac{-E}{kT})$. (**Gibbs-Boltzmann distribution**)

1. We'd like to sample from *Gibbs-B distribution*, which is a task that **Metropolis Algorithm** is an essential method.
   
   Let $\mathbb{C}$ be the set of all states and $S$ be the current state, $N(S)$ be the neighbor states.
   
   In each sampling, we repeat the following algorithm:
   
   ```cpp
   S be the current state
   uniformly sample R from N(S)
   if (E(R)<E(S)) let S be R
   else let S be R with probability exp((E(R)-E(S))/kT)
   ```

Theorem: The stationary distribution is the *Gibbs-Boltzmann distribution*.

2. Finding the state with minimum energy: **SA algorithm**
   
   There is a cooling strategy: $T_1>T_2>\cdots$, and use **Metropolis** algorithm on $T_k$ for certain steps.

## Lecture 14
### Mixing of Markow Chains
Denote that $P$ is the transition matrix of Symmetric Markov Chain. Stationary distribution $\pi$ satisfy $\pi P=\pi$.

$q^{(t)}$ is defined to be $q^{(0)}P^t$, where $q=q^(0)$ is the initial distribution.

**Total vanational distance:** $d_{TV}(a,b)=\frac{1}{2}||a-b||_1$

**Mxing time:** $T(\epsilon)=\sup_q\min\{t\mid d_{TV}(q^{(t)},\pi)\le \epsilon\}$

**Theorem:** $T(\epsilon)\le O(\frac{\ln N+\ln(1/\epsilon)}{1-\lambda_{max}})$, where $\lambda_{max}$ is the modulus of the largest eigenvalue of $P$. Notice that $\pi=(1/N,\cdots,1/N)$.

*proof:* $P=\sum_{i=1}^N \lambda_iv_iv_i^{t}$, where $v_i$s are orthonormal basis, there is fact that $\lambda_{1}=1$ and $v_1=(\frac{1}{\sqrt{N}}, \cdots,\frac{1}{\sqrt{N}})$. Thus if we denote that $q^{(0)}=\sum c_iv_i$, where $c_i=<q^{(0)},v_i>$, then $q^{(t)}=\pi+\sum_{i=2}^N c_i\lambda_i^tv_i$

Therefore
$$||q^{(t)}-\pi||_1\le\sqrt{N}||\sum_{i=2}^Nc_i\lambda _i^t v_i||_2=\sqrt{N\sum_{i=1}^Nc_i^2\lambda_i^{2t}}\le \sqrt{N}\lambda_{max}^t||q^{(0)}||_1$$

### Network Design Game

There is a directed graph $G$ where each edge has a cost $C_e$. There are $k$ player, and player $i$ is required to find a path from $s_i$ to $t_i$. The strategy profile is $P=(P_1,\cdots, P_k)$, and the cost for the $i$ th player is 
$$\sum_{e\in P_i}\frac{C_e}{\#(P_j\text{ that contains }e)}$$
The social cost is $\sum_{i=1}^k{P_i}$

**Theorem:** There is a Nash equilibrium in this game such that the social cost of the NE is $\Theta(\log k)$ times larger than the optimal social cost.

**Price of stability:** $\text{PoS}=\frac{C(\text{best NE})}{C(\text{social optimal})}$

**Price of Anarchy:** $\text{PoA}=\frac{C(\text{worse NE})}{C(\text{social optimal})}$

**Theorem:** $\text{PoS}= O(\log k)$

*proof:* We repeatly heldo **best response dynamics(BRD)** from the socail optimal. 

**Potential function** $\Phi(P)=\sum_{e\in E}{C_e}{\cdot H(x_e)}$, where $x_e$ indicates the number of players that use edge $e$ in $P$, and $H(x)=1+\frac{1}{2}+\cdots+\frac{1}{x}$. We are going to show that this function changes monotonically during the repeat BRD process. This is because $$C_i(P_i,P_{-i})-C_i(P'_i,P_{-i})=\Phi(P_i,P_{-i})-\Phi(P'_i,P_{-i})$$
this supports that the BRD process finally ternimate at a NE state.

Also, notice that 
$$C(NE)=C(P^{(t)})<\Phi(P^{(t)})\le \Phi(P^{(0)})\le H(k)C(P^{(0)})$$ which implies that $\text{PoS}=O(\log k)$

Note that the PoS for undirected graph can be significantly improved to $O(1)$

### Machine scheduling problem

#### Special LP

Recall the LP method that can solve machine scheduling problem:
- $\sum_{j=1}^n(P_{ij}x_{ij})<T$ for arbitrary machine $i$
- $\sum_{i=1}^m x_{ij}=1$, for arbitrary job $j$
- Rounding: $x_{ij}=0$ if $P_{ij}>T$



<!-- If the constraint of a LP problem is 
$$Ax\le b$$
where $A\in\mathbb{R}^{m\times n}$. A vertex solution has at most $m$ nonzero entries. 

Suppose that in a machine scheduling problem, the graph is sparse -->



We construct a bipartite graph $G$ between machines and jobs, and connect $i,j$ when $x_{ij}>0$. We claim that for $A$ is a subset of machine and $B$ is a subset of jobs, then there are at most $|A|+|B|$ edges among $A$ and $B$ in the graph. This is because if we fix the variables other than $x_{ij}$, $i\in A, j\in B$, then the constraints for such $x_{ij}$s is a sub-LP problem and they must reach a vertex solution. Therefore, the number of edges among $A$ and $B$, i.e. the number of nonzero entries does not exceed the number of constraints, which is $|A|+|B|$. 

As a result, $G$ is a collection of trees.

Here we show how to achieve an approximation factor of $2$ for a tree. We simply assume that this tree rooted at a machine. 
- If a job is a leaf, it is assigned to its parent machine
- If a job is a inner node, it is assigned to one of its children.

Therefore, denote that $j'$ is the parent of machine $i$, considered that $x_{ij}=1$ for leaf $j$ such that its parent is $i$. 
$$\text{load of }i\le \sum_{j\text{ is children of }i}P_{ij}+P_{ij'}=\sum_{j\text{ is children of }i}P_{ij}x_{ij}+P_{ij'}\le T+T=2T$$

## Lecture 15
### THM (Hoffman, Kruskal)
Suppose that $A$ is TUM (that is, the determinant of each submatrix $\in\{0,\pm 1\}$), then for arbitrary integral vector $b$
$$P=\{x\mid AX\le b\}$$
is an intergral polyhedron

***Proof***: We only need to show that every vertex solution of $P$ is integral. Note that a definition of vertex solution all $x$ such that $A'x=b'$, where $A'$ is a square submatrix of $X$ and $b'$ is the corresponding subvector of $b$. Thus $x=A'^{-1}b'$, which is integral because $A'$ is integral due to the property of TUM

**Thm (Ghouila-Houri)** 
$A_{m\times n}$ is TUM iff for arbitrary subset $R\subseteq [m]$, there exists a partition $R_1\cup R_2$ of $R$ such that $\sum_{i\in R_1}a_{ij}-\sum_{i\in R_2}a_{ij}\in\{0,\pm 1\}$, for arbitrary $j$.