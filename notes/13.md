## Lecture 13
### Streaming Algorithms
a stream $x_1,\cdots,x_n$, while the space is limited (such as $O(\sqrt{n})$ or $poly\log(n)$)

#### Count Distinct Elements
- The performance of $poly\log$ space and $(\epsilon, \delta)$ approximation can be reached.

**(ideal) hash functions**: Suppose that we have an ideal hash function $h: [m]\rightarrow [0,1]$

Note that if we record the minimum of the hash function result, the expected value of it is $\frac{1}{F_0+1}$, where $F_0$ indicates the number of distinct elements.

Moreover, we maintain the $t=O(\frac{1}{\epsilon^2})$ smallest hash values. Let $v$ be the $t$ th value and output $\tilde{F_0}=\frac{t}{v}$

###### Lemma
$\Pr[\tilde{F_0}<(1-\epsilon)F_0]<\frac{1}{100}$

*Proof*: Let $X_i$ indicates if $h(x_i)\le\frac{t}{F_0(1-\epsilon)}$ and denote that $Y=\sum_{X_i}$, thus we only need to handle $\Pr[Y<t]$

In fact, $\Pr[Y<t]\le\frac{\text{Var}how[Y]}{\epsilon^2t^2}$

#### Bloom Filter

Bloom Filter can handle membership with space much less than the size of set while tradeoff the accuracy.

there are many hash functions $T_1,\cdots, T_k$, each hash value has  a corresponding key boolean value. Whenever $y$ is push into $S$, the key value corresponding to $T_1(h(y)), \cdots, T_k(h(y))$ are set to 1. The output of $y$ is check whether $T_i(h(y))=1$, $\forall i$.

#### min-wise hashing

##### LSH (locality sensitive hashing)

LSH can approximate nearest neihbor search in high dimension

##### Dimension Reduction

Essential algorithm: Johnson-Lindenstrauss

### Local Search

Neiborhood relation over all solutions

local search can be viewed as preforming a walk on the relationship graph

#### Gradient Descent

#### Metropolis Algorithm and Simulated Annealing

Statistical Machanics : $\Pr[a \text{ state with energy } E]\sim \exp(\frac{-E}{kT})$. (**Gibbs-Boltzmann distribution**)

1. We'd like to sample from *Gibbs-B distribution*, which is a task that **Metropolis Algorithm** is an essential method.
   
   Let $\mathbb{C}$ be the set of all states and $S$ be the current state, $N(S)$ be the neighbor states.
   
   In each sampling, we repeat the following algorithm:
   
   ```cpp
   S be the current state
   uniformly sample R from N(S)
   if (E(R)<E(S)) let S be R
   else let S be R with probability exp((E(R)-E(S))/kT)
   ```

Theorem: The stationary distribution is the *Gibbs-Boltzmann distribution*.

2. Finding the state with minimum energy: **SA algorithm**
   
   There is a cooling strategy: $T_1>T_2>\cdots$, and use **Metropolis** algorithm on $T_k$ for certain steps.
